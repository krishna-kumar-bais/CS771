{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> CS771 - Intro to ML (Autumn 2024): Mini-project 1 </h1></center>\n",
    "\n",
    "# Introduction\n",
    "This project involves training four binary classification models with three binary classification datasets, one for each dataset, and one for combined dataset. Each of these datasets represents the same machine learning task and was generated from the same raw dataset. The 3 datasets only differ in terms of features being used to represent each input from the original raw dataset. Each of these 3 datasets further consists of a training set, a validation set, and a test set\n",
    "\n",
    "# Installation\n",
    "\n",
    "```\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "pip install matplotlib\n",
    "pip install sklearn\n",
    "pip install tensorflow.keras\n",
    "```\n",
    "\n",
    "# Emoticon Dataset\n",
    "This Dataset contains 13 emoticons for each sample\n",
    "\n",
    "## Importing the Emoticon Dataset\n",
    "Now we will import the training, and validation set\n",
    "\n",
    "## Taking % of Training Dataset\n",
    "Since we have to check the accuracies when training the model with 20%, 40%, 60%, 80%, and 100% of training data, we will split the training dataset into two ways, once in 20% and 80% and assign them to different variables, and into 40% and 60% and assign them to different variables, so we will have training datasets with 20%, 40%, 60%, 80%, 100% of training data.\n",
    "\n",
    "## For 100% of Training Dataset\n",
    "We will do the entire feature engineering and model prediction for 100% of training set.\n",
    "\n",
    "### Feature transformation and encoding\n",
    "This will be done for both training and validation set, but the training is done with training dataset.\n",
    "- **Transformation**: Since the dataset is in the form of 13 character string, where each character is an emoticon, we will now convert it to 13 featured string where each feature conatins the ord value of the emoticon.\n",
    "\n",
    "- **Encoding**: Now the feature is embeddded using a text based deep learning. After that the features are flattened.\n",
    "\n",
    "- **Standardisation**: This done so that single feature doesn't become too big or small in comparison.\n",
    "\n",
    "### Model Training\n",
    "After the feature engineering, we will now train the model using Support Vector Machine Model, with hyperparameter tuning so that the model provides best accuracy with validation set. Here the model is to trained with training set. With the trained model we will now predict the labels for validation set.\n",
    "\n",
    "### Accuracy Checking\n",
    "With the predicted labels, we will now check the accuracy with the true labels already provided for validation set. We will also display confusion matrix.\n",
    "\n",
    "## For 80% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 80% of the training set.\n",
    "\n",
    "## For 60% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 60% of the training set.\n",
    "\n",
    "## For 40% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 40% of the training set.\n",
    "\n",
    "## For 20% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 20% of the training set.\n",
    "\n",
    "## Accuracy Variation plot for different % of data\n",
    "Now we will plot a line graph of the accuries of the model with different percentage of training set.\n",
    "\n",
    "## Prediction for test data\n",
    "We will import test set and then we will apply the same feature engineering used for training and validation set, and use the model trained with training set to predict the test set labels.\n",
    "\n",
    "# Deep Feature Dataset\n",
    "This dataset a matrix of shape (13,768) for each sample.\n",
    "\n",
    "## Importing the Deep feature Dataset\n",
    "Now we will import the training, and validation set\n",
    "\n",
    "## Taking % of Training Dataset\n",
    "Since we have to check the accuracies when training the model with 20%, 40%, 60%, 80%, and 100% of training data, we will split the training dataset into two ways, once in 20% and 80% and assign them to different variables, and into 40% and 60% and assign them to different variables, so we will have training datasets with 20%, 40%, 60%, 80%, 100% of training data.\n",
    "\n",
    "## For 100% of Training Dataset\n",
    "We will do the entire feature engineering and model prediction for 100% of training set.\n",
    "\n",
    "### Feature transformation\n",
    "This will be done for both training and validation set, but the training is done with training dataset. Here first the feature are flattened and then Log Function transformer is applied to normalise the data.\n",
    "\n",
    "### Feature Reduction\n",
    "This will be done for both training and validation set, but the training is done with training dataset. After feature transformation, number of features will be too large. so we reduced it using Principle Component Analysis.\n",
    "\n",
    "### Model Training\n",
    "After the feature engineering, we will now train the model using Support Vector Machine Model, with hyperparameter tuning so that the model provides best accuracy with validation set. Here the model is to trained with training set. With the trained model we will now predict the labels for validation set.\n",
    "\n",
    "### Accuracy Checking\n",
    "With the predicted labels, we will now check the accuracy with the true labels already provided for validation set. We will also display confusion matrix.\n",
    "\n",
    "## For 80% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 80% of the training set.\n",
    "\n",
    "## For 60% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 60% of the training set.\n",
    "\n",
    "## For 40% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 40% of the training set.\n",
    "\n",
    "## For 20% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 20% of the training set.\n",
    "\n",
    "## Accuracy Variation plot for different % of data\n",
    "Now we will plot a line graph of the accuries of the model with different percentage of training set.\n",
    "\n",
    "## Prediction for test data\n",
    "We will import test set and then we will apply the same feature engineering used for training and validation set, and use the model trained with training set to predict the test set labels.\n",
    "\n",
    "# Text Sequence Dataset\n",
    "In this Dataset each sample is of the form of a string of length 50, where each character is a digit.\n",
    "\n",
    "## Importing the Text Sequence Dataset\n",
    "Now we will import the training, and validation set\n",
    "\n",
    "## Taking % of Training Dataset\n",
    "Since we have to check the accuracies when training the model with 20%, 40%, 60%, 80%, and 100% of training data, we will split the training dataset into two ways, once in 20% and 80% and assign them to different variables, and into 40% and 60% and assign them to different variables, so we will have training datasets with 20%, 40%, 60%, 80%, 100% of training data.\n",
    "\n",
    "## For 100% of Training Dataset\n",
    "We will do the entire feature engineering and model prediction for 100% of training set.\n",
    "\n",
    "## Feature Transformation and encoding\n",
    "This will be done for both training and validation set, but the training is done with training dataset.\n",
    "- **Transformation**: We will transform the feature to resemble the emoticon dataset after its feature transformation, and this possible as we can have a bijective mapping between emoticon and text sequence dataset. This can done without using the emoticon dataset, by stricly observing anf following the rules of the pattern.\n",
    "\n",
    "- **Encoding**: Now the feature is embeddded using a text based deep learning. After that the features are flattened.\n",
    "\n",
    "- **Standardisation**: This done so that single feature doesn't become too big or small in comparison.\n",
    "\n",
    "### Model Training\n",
    "After the feature engineering, we will now train the model using Support Vector Machine Model, with hyperparameter tuning so that the model provides best accuracy with validation set. Here the model is to trained with training set. With the trained model we will now predict the labels for validation set.\n",
    "\n",
    "### Accuracy Checking\n",
    "With the predicted labels, we will now check the accuracy with the true labels already provided for validation set. We will also display confusion matrix.\n",
    "\n",
    "## For 80% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 80% of the training set.\n",
    "\n",
    "## For 60% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 60% of the training set.\n",
    "\n",
    "## For 40% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 40% of the training set.\n",
    "\n",
    "## For 20% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 20% of the training set.\n",
    "\n",
    "## Accuracy Variation plot for different % of data\n",
    "Now we will plot a line graph of the accuries of the model with different percentage of training set.\n",
    "\n",
    "## Prediction for test data\n",
    "We will import test set and then we will apply the same feature engineering used for training and validation set, and use the model trained with training set to predict the test set labels.\n",
    "\n",
    "# Combine Dataset\n",
    "In this we will only use emoticon and deep feature dataset, as emoticon and text sequence dataset are virtually the same, and no need for us to repeat it twice.\n",
    "\n",
    "## Importing the combine dataset\n",
    "Now we will import the training, and validation set, for both emoticon and deep feature dataset.\n",
    "\n",
    "## Taking % of Training Dataset\n",
    "Since we have to check the accuracies when training the model with 20%, 40%, 60%, 80%, and 100% of training data, we will split the training dataset into two ways, once in 20% and 80% and assign them to different variables, and into 40% and 60% and assign them to different variables, so we will have training datasets with 20%, 40%, 60%, 80%, 100% of training data.\n",
    "\n",
    "## For 100% of Training Dataset\n",
    "We will do the entire feature engineering and model prediction for 100% of training set.\n",
    "\n",
    "### Feature Transformation and Encoding\n",
    "We will do feature engineering separately for emoticon and deep feature dataset\n",
    "- **Emoticon Dataset:** The feature engineering here will be same as the one done before, no changes\n",
    "- **Deep feature dataset:** The feature engineering here will be same as the one done before, no changes\n",
    "\n",
    "### Model Training\n",
    "After the feature engineering, we will now train the model using Ensemble or Stacking Model, for combining the dataset, get a meta feature and train a new random forst classifier model. This model is  then used to predict labels for the validation set.\n",
    "\n",
    "### Accuracy Checking\n",
    "With the predicted labels, we will now check the accuracy with the true labels already provided for validation set. We will also display confusion matrix.\n",
    "\n",
    "## For 80% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 80% of the training set.\n",
    "\n",
    "## For 60% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 60% of the training set.\n",
    "\n",
    "## For 40% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 40% of the training set.\n",
    "\n",
    "## For 20% of Training Dataset\n",
    "All the feature engineering and model training will be same as the one done with 100% training data, except here we will only use 20% of the training set.\n",
    "\n",
    "## Accuracy Variation plot for different % of data\n",
    "Now we will plot a line graph of the accuries of the model with different percentage of training set.\n",
    "\n",
    "## Prediction for test data\n",
    "We will import test set and then we will apply the same feature engineering used for training and validation set, and use the model trained with training set to predict the test set labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
