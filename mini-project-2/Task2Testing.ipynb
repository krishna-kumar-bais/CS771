{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, AveragePooling2D\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from keras.applications.inception_v3 import preprocess_input as incv3_preprocess_input\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data and feature extraction using pretrained Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (299,299,3)\n",
    "\n",
    "def create_model_incv3():\n",
    "    tf_input = Input(shape=input_shape)\n",
    "    model = InceptionV3(input_tensor=tf_input, weights='imagenet', include_top=False)\n",
    "    output_pooled = AveragePooling2D((8, 8), strides=(8, 8))(model.output)\n",
    "    return Model(model.input, output_pooled)\n",
    "\n",
    "create_model = create_model_incv3\n",
    "\n",
    "batch_of_images_placeholder = tf.TensorSpec(shape=(None, 32, 32, 3), dtype=tf.uint8)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "tf_resize_op = lambda images: tf.image.resize(tf.cast(images, tf.float32), input_shape[:2])\n",
    "\n",
    "preprocess_input = incv3_preprocess_input\n",
    "\n",
    "# Define generator\n",
    "def data_generator(data, labels=None):\n",
    "    def generator():\n",
    "        n = data.shape[0]\n",
    "        start = 0\n",
    "        while start < n:\n",
    "            end = min(start + batch_size, n)\n",
    "            batch_of_images = data[start:end]\n",
    "            batch_of_images_resized = tf_resize_op(batch_of_images)\n",
    "            batch_of_images_preprocessed = preprocess_input(batch_of_images_resized)\n",
    "            if labels is not None:\n",
    "                batch_of_labels = labels[start:end]\n",
    "            else:\n",
    "                # Create dummy labels as a placeholder when no labels are provided\n",
    "                batch_of_labels = np.zeros((batch_of_images_preprocessed.shape[0],))\n",
    "            yield (batch_of_images_preprocessed, batch_of_labels)\n",
    "            start += batch_size\n",
    "    return generator\n",
    "\n",
    "model_extract = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7932/2421634456.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  t_train = torch.load(f'dataset/part_two_dataset/train_data/{I}_train_data.tar.pth')\n",
      "/tmp/ipykernel_7932/2421634456.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  t_eval = torch.load(f'dataset/part_two_dataset/eval_data/{I}_eval_data.tar.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 779ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 749ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 788ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 782ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 790ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 845ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 784ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 777ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 814ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 829ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 782ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 858ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 943ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 826ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 822ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 866ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 867ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 842ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 835ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 830ms/step\n"
     ]
    }
   ],
   "source": [
    "def extract_features(I, model_extract, data_generator, batch_size):\n",
    "    # Load the training and evaluation datasets using `I`\n",
    "    t_train = torch.load(f'dataset/part_two_dataset/train_data/{I}_train_data.tar.pth')\n",
    "    x_train = t_train['data']  # numpy.ndarray\n",
    "    t_eval = torch.load(f'dataset/part_two_dataset/eval_data/{I}_eval_data.tar.pth')\n",
    "    x_eval, y_eval = t_eval['data'], t_eval['targets']  # numpy.ndarray\n",
    "\n",
    "    # Convert data to numpy arrays\n",
    "    x_train = np.array(x_train)\n",
    "    x_eval = np.array(x_eval)\n",
    "    y_eval = np.array(y_eval).flatten()  # Flatten y_eval for consistency\n",
    "\n",
    "    # Get number of samples in training and evaluation sets\n",
    "    n_training = x_train.shape[0]\n",
    "    n_evaling = y_eval.shape[0]\n",
    "\n",
    "    # Generate data for model extraction\n",
    "    data_train_gen = data_generator(x_train, None)()\n",
    "    ftrs_training = model_extract.predict(data_train_gen, steps=math.ceil(n_training / batch_size), verbose=1)\n",
    "\n",
    "    data_eval_gen = data_generator(x_eval, None)()\n",
    "    ftrs_evaling = model_extract.predict(data_eval_gen, steps=math.ceil(n_evaling / batch_size), verbose=1)\n",
    "\n",
    "    # Extract features by flattening predictions\n",
    "    x_train_extracted = np.array([ftrs_training[i].flatten() for i in range(n_training)])\n",
    "    x_eval_extracted = np.array([ftrs_evaling[i].flatten() for i in range(n_evaling)])\n",
    "\n",
    "    return x_train_extracted, x_eval_extracted, y_eval\n",
    "\n",
    "for I in range(1,11):\n",
    "    globals()[f\"x_train_extracted_{I+10}\"], globals()[f\"x_eval_extracted_{I+10}\"], globals()[f\"y_eval_{I+10}\"] = extract_features(\n",
    "        I=I,\n",
    "        model_extract=model_extract,\n",
    "        data_generator=data_generator,\n",
    "        batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving extracted features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11, 21):\n",
    "    feature = eval(f\"x_train_extracted_{i}\")\n",
    "    folder_path = \"extracted_features\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    file_name = f\"x_train_extracted_{i}.npy\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    np.save(file_path, feature)\n",
    "\n",
    "for i in range(11, 21):\n",
    "    feature = eval(f\"x_eval_extracted_{i}\")\n",
    "    folder_path = \"extracted_features\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    file_name = f\"x_eval_extracted_{i}.npy\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    np.save(file_path, feature)\n",
    "\n",
    "for i in range(11, 21):\n",
    "    feature = eval(f\"y_eval_{i}\")\n",
    "    folder_path = \"extracted_features\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    file_name = f\"y_eval_{i}.npy\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    np.save(file_path, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Saved Extracted features from current task and Evaluation features from Previous Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,21):\n",
    "    globals()[f'x_eval_extracted_{i}'] = np.load(f'extracted_features/x_eval_extracted_{i}.npy')\n",
    "    globals()[f'y_eval_{i}'] = np.load(f'extracted_features/y_eval_{i}.npy')\n",
    "\n",
    "for i in range(11,21):\n",
    "    globals()[f'x_train_extracted_{i}'] = np.load(f'extracted_features/x_train_extracted_{i}.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Classifier Continual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeClassifierContinual:\n",
    "    def __init__(self, n_classes, n_features, alpha=0.5):\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.means = np.zeros((n_classes, n_features))\n",
    "        self.covariances = np.array([np.eye(n_features) for _ in range(n_classes)])\n",
    "        self.alpha = alpha  # Regularization weight\n",
    "\n",
    "    def fit(self, X_train, y_train, prev_means=None, prev_covariances=None):\n",
    "        new_means = np.zeros((self.n_classes, self.n_features))\n",
    "        new_covariances = np.array([np.eye(self.n_features) for _ in range(self.n_classes)])\n",
    "\n",
    "        # Calculate Gaussian parameters for current data\n",
    "        for c in range(self.n_classes):\n",
    "            class_samples = X_train[y_train == c]\n",
    "            if len(class_samples) > 0:\n",
    "                new_means[c] = np.mean(class_samples, axis=0)\n",
    "                if len(class_samples) > 1:\n",
    "                    new_covariances[c] = np.cov(class_samples, rowvar=False) + 1e-6 * np.eye(self.n_features)\n",
    "\n",
    "        # Regularize means and covariances with the previous model's parameters\n",
    "        if prev_means is not None and prev_covariances is not None:\n",
    "            self.means = self.alpha * prev_means + (1 - self.alpha) * new_means\n",
    "            self.covariances = self.alpha * prev_covariances + (1 - self.alpha) * new_covariances\n",
    "        else:\n",
    "            self.means = new_means\n",
    "            self.covariances = new_covariances\n",
    "\n",
    "    def generate_replay_data(self, num_samples_per_class=50):\n",
    "        # Generate synthetic data based on current Gaussian parameters\n",
    "        replay_data = []\n",
    "        replay_labels = []\n",
    "        for c in range(self.n_classes):\n",
    "            samples = np.random.multivariate_normal(self.means[c], self.covariances[c], num_samples_per_class)\n",
    "            replay_data.append(samples)\n",
    "            replay_labels.append(np.full(num_samples_per_class, c))\n",
    "        return np.vstack(replay_data), np.concatenate(replay_labels)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Calculate log probabilities for each class\n",
    "        log_probs = np.zeros((X.shape[0], self.n_classes))\n",
    "        for c in range(self.n_classes):\n",
    "            distribution = multivariate_normal(mean=self.means[c], cov=self.covariances[c])\n",
    "            log_probs[:, c] = distribution.logpdf(X)\n",
    "\n",
    "        # Assign each sample to the class with the highest log probability\n",
    "        predictions = np.argmax(log_probs, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the models and transformation saved from previous task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib  # Or use pickle if saved with it\n",
    "\n",
    "model_list = joblib.load('model_list.joblib')\n",
    "scaler = joblib.load('scaler.joblib')\n",
    "pca = joblib.load('pca.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=10\n",
    "pca_components=40\n",
    "\n",
    "# Continual learning with Gaussian class-conditionals, prototype regularization, and generative replay\n",
    "for i in range(11, 21):\n",
    "    # Standardize and apply PCA to the current dataset\n",
    "    x_train_scaled = scaler.transform(globals()[f'x_train_extracted_{i}'])\n",
    "    x_train_pca = pca.transform(x_train_scaled)\n",
    "\n",
    "    # Generate replay data using the previous model\n",
    "    replay_data, replay_labels = model_list[-1].generate_replay_data(num_samples_per_class=50)\n",
    "\n",
    "    # Combine replay data with current data\n",
    "    x_combined = np.vstack([x_train_pca, replay_data])\n",
    "    y_combined = np.concatenate([model_list[-1].predict(x_train_pca), replay_labels])\n",
    "\n",
    "    # Train a new model with regularized Gaussian parameters\n",
    "    model = GenerativeClassifierContinual(n_classes, n_features=pca_components, alpha=0.7)\n",
    "    model.fit(x_combined, y_combined,\n",
    "              prev_means=model_list[-1].means, prev_covariances=model_list[-1].covariances)\n",
    "\n",
    "    # Append the new model to the model list\n",
    "    model_list.append(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy checking of different Models with different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Matrix:\n",
      "          Eval Dataset 1  Eval Dataset 2  Eval Dataset 3  Eval Dataset 4  \\\n",
      "Model 11           78.20           78.04           76.80           77.36   \n",
      "Model 12           77.36           77.56           76.92           77.04   \n",
      "Model 13           76.44           77.80           75.80           76.24   \n",
      "Model 14           76.52           77.44           75.68           76.08   \n",
      "Model 15           76.52           76.88           75.48           76.08   \n",
      "Model 16           75.72           76.48           74.72           75.56   \n",
      "Model 17           75.76           76.40           74.72           75.36   \n",
      "Model 18           75.00           76.08           74.64           74.56   \n",
      "Model 19           74.84           76.16           74.24           74.12   \n",
      "Model 20           74.44           76.00           74.08           73.76   \n",
      "\n",
      "          Eval Dataset 5  Eval Dataset 6  Eval Dataset 7  Eval Dataset 8  \\\n",
      "Model 11           77.44           77.80           77.12           78.36   \n",
      "Model 12           76.92           77.20           76.72           77.84   \n",
      "Model 13           76.36           76.72           76.16           76.92   \n",
      "Model 14           76.44           76.24           76.20           77.12   \n",
      "Model 15           75.88           75.96           75.60           76.84   \n",
      "Model 16           75.68           75.12           75.08           76.04   \n",
      "Model 17           75.36           74.72           74.84           75.72   \n",
      "Model 18           75.08           74.76           74.60           75.56   \n",
      "Model 19           74.52           74.56           74.52           75.04   \n",
      "Model 20           74.60           74.28           74.24           75.24   \n",
      "\n",
      "          Eval Dataset 9  Eval Dataset 10  Eval Dataset 11  Eval Dataset 12  \\\n",
      "Model 11           76.44            77.32            55.48            41.76   \n",
      "Model 12           76.04            77.08            54.68            43.60   \n",
      "Model 13           75.44            76.36            54.44            43.56   \n",
      "Model 14           75.44            76.16            53.40            42.96   \n",
      "Model 15           75.00            75.56            53.24            42.96   \n",
      "Model 16           74.24            75.36            52.64            42.76   \n",
      "Model 17           73.88            75.40            53.04            42.08   \n",
      "Model 18           73.72            75.16            52.68            42.00   \n",
      "Model 19           73.44            75.36            52.24            42.16   \n",
      "Model 20           73.36            74.68            51.92            41.84   \n",
      "\n",
      "          Eval Dataset 13  Eval Dataset 14  Eval Dataset 15  Eval Dataset 16  \\\n",
      "Model 11            60.56            73.52            77.08            57.48   \n",
      "Model 12            60.36            73.28            76.40            56.08   \n",
      "Model 13            60.28            72.44            75.92            57.00   \n",
      "Model 14            60.24            72.44            75.48            56.40   \n",
      "Model 15            59.60            72.44            75.40            55.88   \n",
      "Model 16            59.64            72.08            74.88            56.44   \n",
      "Model 17            59.40            72.08            74.88            56.28   \n",
      "Model 18            59.52            71.68            74.12            55.36   \n",
      "Model 19            58.88            71.24            73.92            55.68   \n",
      "Model 20            58.92            70.84            73.64            55.48   \n",
      "\n",
      "          Eval Dataset 17  Eval Dataset 18  Eval Dataset 19  Eval Dataset 20  \n",
      "Model 11            62.92            61.32            55.32            74.04  \n",
      "Model 12            62.28            61.00            55.16            73.16  \n",
      "Model 13            61.72            61.28            54.60            72.96  \n",
      "Model 14            61.48            60.76            54.68            72.88  \n",
      "Model 15            61.04            60.28            54.64            72.28  \n",
      "Model 16            60.60            60.08            54.04            71.72  \n",
      "Model 17            62.04            59.92            53.88            71.88  \n",
      "Model 18            61.36            60.16            53.44            71.72  \n",
      "Model 19            60.84            59.92            55.12            71.48  \n",
      "Model 20            60.64            59.48            55.04            71.72  \n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model on the held-out datasets\n",
    "accuracy_matrix = np.zeros((10, 20))\n",
    "for i, model in enumerate(model_list[10:], start=11):\n",
    "    for j in range(1, 21):\n",
    "        x_eval_scaled = scaler.transform(globals()[f'x_eval_extracted_{j}'])\n",
    "        x_eval_pca = pca.transform(x_eval_scaled)\n",
    "        eval_labels = globals()[f'y_eval_{j}']\n",
    "        accuracy_matrix[i-11, j-1] = model.score(x_eval_pca, eval_labels) * 100\n",
    "\n",
    "print(\"Accuracy Matrix:\")\n",
    "\n",
    "accuracy_matrix = pd.DataFrame(\n",
    "    accuracy_matrix,\n",
    "    index=[f\"Model {i}\" for i in range(11,21)],\n",
    "    columns=[f\"Eval Dataset {j+1}\" for j in range(20)]\n",
    ")\n",
    "\n",
    "print(accuracy_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
